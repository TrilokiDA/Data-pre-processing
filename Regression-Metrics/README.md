The major Regression metrics
---

### ğŸ”¹ 1. **Mean Absolute Error (MAE)**

#### âœ… Why we use it:

MAE measures the **average magnitude** of the errors in a set of predictions, without considering their direction (i.e., positive or negative). It tells us how far our predictions are from actual values on average.

#### ğŸ” How it's calculated:

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} | y_i - \hat{y}_i |
$$

* $y_i$: Actual value
* $\hat{y}_i$: Predicted value
* $n$: Number of observations

#### ğŸ“Œ Where to use:

* When you want a metric thatâ€™s **robust to outliers** and **easily interpretable**.
* Used in business applications where the cost of an error is linear (e.g., predicting delivery time).

#### ğŸ”— Related to others:

* MAE is **less sensitive to outliers** compared to RMSE (Root Mean Squared Error).
* Always non-negative and **in the same unit** as the output variable.

---

### ğŸ”¹ 2. **Mean Squared Error (MSE)**

#### âœ… Why we use it:

MSE measures the **average squared difference** between predicted and actual values. It **penalizes larger errors** more than MAE, making it useful when large errors are especially undesirable.

#### ğŸ” How it's calculated:

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

#### ğŸ“Œ Where to use:

* When you want to **heavily penalize large errors**.
* Common in optimization problems because it's **differentiable**, making it suitable for gradient-based algorithms like linear regression.

#### ğŸ”— Related to others:

* MSE is the **square** of RMSE.
* Compared to MAE, itâ€™s **more sensitive to outliers**.
* Same unit as the **square** of the target variable (not interpretable in original unit).

---

### ğŸ”¹ 3. **Root Mean Squared Error (RMSE)**

#### âœ… Why we use it:

RMSE is the **square root of MSE**, which brings the error back to the original unit of the target variable, making it easier to interpret.

#### ğŸ” How it's calculated:

$$
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{ \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 }
$$

#### ğŸ“Œ Where to use:

* When interpretability in the same unit as the output is important.
* Useful in domains like real estate (predicting house prices), energy demand, etc.

#### ğŸ”— Related to others:

* RMSE **amplifies the effect of outliers** more than MAE.
* RMSE â‰¥ MAE (always true except when all errors are the same).

---

### ğŸ”¹ 4. **Mean Absolute Percentage Error (MAPE)**

#### âœ… Why we use it:

MAPE expresses the error **as a percentage** of the actual values, making it **scale-independent** and easy to interpret.

#### ğŸ” How it's calculated:

$$
\text{MAPE} = \frac{100\%}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|
$$

#### ğŸ“Œ Where to use:

* When comparing models across datasets with **different scales**.
* Business forecasts (e.g., sales, demand), where % error is intuitive.

#### âš ï¸ Pitfalls:

* Undefined when $y_i = 0$
* Sensitive to small actual values

#### ğŸ”— Related to others:

* Unlike MAE/RMSE, MAPE is **relative**, not absolute.
* Used for performance **comparison across units**.

---

### ğŸ”¹ 5. **Symmetric Mean Absolute Percentage Error (sMAPE)**

#### âœ… Why we use it:

sMAPE adjusts MAPE to **symmetrically** treat over- and under-forecasting, especially useful when actual values are near zero.

#### ğŸ” How it's calculated:

$$
\text{sMAPE} = \frac{100\%}{n} \sum_{i=1}^{n} \frac{|y_i - \hat{y}_i|}{(|y_i| + |\hat{y}_i|)/2}
$$

#### ğŸ“Œ Where to use:

* Time series forecasting
* Better than MAPE when $y_i$ can be close to 0

#### ğŸ”— Related to others:

* An improvement over MAPE in handling zero or near-zero values.
* Still suffers when both actual and predicted are small.

---

### ğŸ”¹ 6. **R-squared (RÂ² Score)**

#### âœ… Why we use it:

RÂ² measures the **proportion of variance explained** by the model. It's a **goodness-of-fit** measure.

#### ğŸ” How it's calculated:

$$
R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
$$

* $\bar{y}$: Mean of actual values

#### ğŸ“Œ Where to use:

* When you want to understand how much variability in the target is captured by the model.
* Useful for model comparison.

#### ğŸ”— Related to others:

* RÂ² ranges from $-\infty$ to 1.
* 1 = perfect fit, 0 = no better than mean, negative = worse than mean.
* Based on **MSE** in the numerator and total variance in the denominator.

---

### ğŸ”¹ 7. **Adjusted R-squared**

#### âœ… Why we use it:

Adjusted RÂ² adjusts the RÂ² score for the **number of predictors**, preventing overestimation of model quality when adding irrelevant features.

#### ğŸ” How it's calculated:

$$
\text{Adjusted } R^2 = 1 - \left( \frac{(1 - R^2)(n - 1)}{n - p - 1} \right)
$$

* $n$: Number of observations
* $p$: Number of predictors

#### ğŸ“Œ Where to use:

* In multiple linear regression
* When comparing models with **different numbers of features**

#### ğŸ”— Related to others:

* Always **â‰¤ RÂ²**
* Rewards useful features, **penalizes** unnecessary ones

---

### ğŸ”¹ 8. **Huber Loss**

#### âœ… Why we use it:

Huber loss combines advantages of MAE and MSE: it's **quadratic** for small errors and **linear** for large ones â€” making it **robust to outliers** while still differentiable.

#### ğŸ” How it's calculated:

$$
L_\delta(a) = 
\begin{cases} 
\frac{1}{2}a^2 & \text{for } |a| \leq \delta \\
\delta(|a| - \frac{1}{2}\delta) & \text{otherwise}
\end{cases}
$$

* $a = y_i - \hat{y}_i$

#### ğŸ“Œ Where to use:

* When you want a **robust and smooth** loss function in optimization.
* Often used in robust regression models.

#### ğŸ”— Related to others:

* Transitions from **MSE** (near center) to **MAE** (far from center).
* Requires tuning $\delta$ threshold.

---

### ğŸ”¹ 9. **Mean Bias Deviation (MBE)**

#### âœ… Why we use it:

MBE gives the **average error with sign**, telling if the model tends to **overpredict or underpredict**.

#### ğŸ” How it's calculated:

$$
\text{MBE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)
$$

#### ğŸ“Œ Where to use:

* In **energy modeling**, environmental modeling, or when **bias direction** matters.
* Indicates model tendency, not accuracy.

#### ğŸ”— Related to others:

* Not typically used alone â€” often paired with RMSE or MAE.

---

### ğŸ”¹ 10. **Relative Absolute Error (RAE)**

#### âœ… Why we use it:

RAE compares the **total absolute error** of the model to that of a naive model that predicts the mean of actuals.

#### ğŸ” How it's calculated:

$$
\text{RAE} = \frac{\sum |y_i - \hat{y}_i|}{\sum |y_i - \bar{y}|}
$$

#### ğŸ“Œ Where to use:

* To evaluate model improvement over a baseline (e.g., mean predictor).

#### ğŸ”— Related to others:

* If RAE < 1, model is better than mean.
* Closely linked to MAE and RÂ².

---

### ğŸ”¹ 11. **Relative Squared Error (RSE)**

#### âœ… Why we use it:

RSE compares **squared errors** to a naive predictor's squared error.

#### ğŸ” How it's calculated:

$$
\text{RSE} = \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}
$$

#### ğŸ“Œ Where to use:

* When comparing models to a baseline using **squared loss**.

#### ğŸ”— Related to others:

* Inverse of $R^2$:

  $$
  R^2 = 1 - RSE
  $$

---

### ğŸ”š Summary Table

| Metric     | Penalizes Outliers | Scale Dependent | Interpretable | Robust to Outliers |
| ---------- | ------------------ | --------------- | ------------- | ------------------ |
| MAE        | No                 | Yes             | Yes           | âœ…                  |
| MSE        | âœ…                  | Yes             | No            | âŒ                  |
| RMSE       | âœ…âœ…                 | Yes             | âœ…             | âŒ                  |
| MAPE       | âŒ                  | No              | âœ…             | âŒ                  |
| sMAPE      | âŒ                  | No              | âœ…             | âŒ                  |
| RÂ²         | No                 | No              | Partial       | âŒ                  |
| Adj. RÂ²    | No                 | No              | Partial       | âŒ                  |
| Huber Loss | âœ…                  | Yes             | No            | âœ…                  |
| MBE        | No                 | Yes             | Partial       | âŒ                  |
| RAE/RSE    | No                 | No              | Partial       | âŒ                  |


---

## ğŸ§© Why This Is Important:

Not all metrics are equally useful in every regression task. The **best metric** often depends on:

| Factor                     | Why It Matters                                                                        |
| -------------------------- | ------------------------------------------------------------------------------------- |
| **Scale of the target**    | Some metrics (like MAPE) donâ€™t work well if values are near zero                      |
| **Presence of outliers**   | Some metrics (like RMSE) are very sensitive to outliers; others (like MAE) are robust |
| **Business goal**          | Are you minimizing large errors? Is direction of bias important?                      |
| **Interpretability needs** | Some metrics are easier to explain to stakeholders                                    |
| **Model comparison**       | Relative metrics help compare to a baseline (like RÂ², RAE, RSE)                       |

---

## âœ… Example 1: Predicting House Prices

* **Target:** House price in â‚¹
* **Problem nature:** Large range, outliers possible
* **Best metrics:**

  * RMSE â†’ To heavily penalize large price prediction errors
  * MAE â†’ For interpretable average error
  * RÂ² â†’ For model goodness-of-fit

---

## âœ… Example 2: Forecasting Energy Demand

* **Target:** kWh usage (can be close to zero)
* **Problem nature:** Forecasting time series
* **Best metrics:**

  * sMAPE â†’ Handles % error better when values are near zero
  * MAE â†’ Stable across forecast horizon
  * MBE â†’ To check if the model over- or under-predicts

---

## âœ… Example 3: Predicting Sales Volume for Inventory

* **Target:** Number of items sold (e.g., 0â€“100)
* **Problem nature:** Many small values, no outliers
* **Best metrics:**

  * MAPE â†’ Good because % error is meaningful
  * MAE â†’ Direct interpretation
  * Adjusted RÂ² â†’ For comparing models with different features

---
